In this chapter we will discuss the datasets used for the evaluation and the results obtained by the algorithm.

\section{Datasets}

The datasets used for the comparison were retrieved from the UCI dataset repository \cite{UCI}. The datasets chosen are all exclusively categorical without any missing values, as those are the two aspects that prism cannot handle well. The datasets chosen are the Car Evaluation Data Set, kr-vs-kp and hayes-roth datasets.

\subsection{Car Evaluation Data Set}

Car Evaluation Database\cite{CarDatasetUCI} was derived from a simple hierarchical decision model originally developed for the demonstration of DEX, M. Bohanec, V. Rajkovic: Expert system for decision making. Sistemica 1(1), pp. 145-157, 1990.).\;

Input attributes are printed in lowercase. Besides the target concept (CAR), the model includes three intermediate concepts: PRICE, TECH, COMFORT. Every concept is in the original model related to its lower level descendants by a set of examples.\;

The Car Evaluation Database contains examples with the structural information removed, i.e., directly relates CAR to the six input attributes: buying, maint, doors, persons, lug\_boot, safety.\;

Because of known underlying concept structure, this database may be particularly useful for testing constructive induction and structure discovery methods.\;

The characteristics of the dataset are as shown in \ref{table:Car-Evaluation}\;
\begin{table}[ht]
    \resizebox{\textwidth}{!}{
        \begin{tabular}{||c|c||c|c||c|c||}
            Data Set Characteristics: & Multivariate & Number of Instances: & 1728 & Area: & N/A \\
            \hline
            \hline
            Attribute Characteristics: & Categorical & Number of Attributes: & 6 & Date Donated & 1997-06-01 \\
            \hline
            \hline
            Associated Tasks: & Classification & Missing Values & No & Number of Web Hits: & 1347720 \\
        \end{tabular}
    }
\caption{\label{table:Car-Evaluation}Car Evaluation Characteristics}
\end{table}

\subsection{Hayes-Roth Data Set}

The Hayes-Roth Data Set\cite{HayesRothDatasetUCI} contains 5 numeric-valued attributes. Only a subset of 3 are used during testing (the latter 3). Furthermore, only 2 of the 3 concepts are "used" during testing (i.e., those with the prototypes 000 and 111). The Characteristics of the dataset are as shown in \ref{table:Hayes-Roth}\;

\begin{table}[ht]
    \resizebox{\textwidth}{!}{
        \begin{tabular}{||c|c||c|c||c|c||}
            Data Set Characteristics: & Multivariate & Number of Instances: & 160 & Area: & Social \\
            \hline
            \hline
            Attribute Characteristics: & Categorical & Number of Attributes: & 5 & Date Donated & 1989-03-01 \\
            \hline
            \hline
            Associated Tasks: & Classification & Missing Values? & No & Number of Web Hits: & 108367 \\
        \end{tabular}
    }
\caption{\label{table:Hayes-Roth}Hayes-Roth Characteristics}
\end{table}

\subsection{Chess (King-Rook vs. King-Pawn) Data Set}

The last dataset used in the project is the Chess (King-Rook vs. King-Pawn) Data Set \cite{KPvsKRDatasetUCI}, which consists of chess data from the match. The Dataset characteristics are shown in \ref{table:krvskp}.

\begin{table}[ht]
    \resizebox{\textwidth}{!}{
        \begin{tabular}{||c|c||c|c||c|c||}
            Data Set Characteristics: & Multivariate & Number of Instances: & 3196 & Area: & Game \\
            \hline
            \hline
            Attribute Characteristics: & Categorical & Number of Attributes: & 36 & Date Donated & 1989-08-01 \\
            \hline
            \hline
            Associated Tasks: & Classification & Missing Values? & No & Number of Web Hits: & 125000 \\
        \end{tabular}
    }
\caption{\label{table:krvskp}Chess (King-Rook vs. King-Pawn) Characteristics}
\end{table}

\section{Test Data}

The Hayes-Roth dataset already provides a predefined train-test split of the data in the database, however, both the car and chess datasets do not have this partitions. Because of that, a partition script has been created in \path{./scripts/split_train_test.py} where a dataset can be loaded with the \emph{-i} flag and a train percentage can be epecified with the \emph{-t} flag. The remaining of the data will go to a test split. If no train percentage is used, the script defaults to $.7$ train ratio.\;

The script segments the dataset by class and extracts the train percentage from the class instances. By doing the split this way we achieve partitions with the same class distribution and class balance as the original dataset in both train and test split

\section{Training Results}

The extraction script for the rules will save the inferend rules and the prism object attributes to a folder which by default is \path{./models/}. The rules are saved in .rules format as plain text and will be presented in the following sections.

The format for the rules is as follows in all three cases: "IF $attr_0$ IS $value_0$ AND ... THEN $class$ IS $label\ (accuracy,\ coverage)$". We present the rules in ascending order of number of rules. All the rules are sorted by decreasing accuracy and coverage.

\subsection{Hayes-Roth Data Set}

The hayes-roth dataset yielded 39 rules. 21 of them have perfect accuracy and were selected first. By looking at the dataset it can be noted that the antecedent of the rules with accuracy != 1.0 match the conflicting items in the dataset, where two or more instances have the same antecedent, but different class label. Because of that, it can be seen that p.e. rule \#22 and rule \#37 have the same antecedent but different label. They both have a coverage of 3 instances but rule \#22 has an accuracy of 2/3 and rule \#37 has an accuracy of 1/3 which means that two instances that satisfy the antecedent have class $1$ and one has class $2$.\;

The accuracy yielded in the training dataset was 88.64\% and the coverage is 100\%.

\lstinputlisting[]{../../models/hayes-roth.rules}

\subsection{Chess (King-Rook vs. King-Pawn) Data Set}

The chess dataset yielded 88 rules. All of them have perfect accuracy and almost all of them have a coverage > 1 which means that englobe more than one instance of the dataset in one rule. The first 14 rules alone cover a big part of the dataset as the first one alone covers 499 instances in the dataset, which entiles 15.61\% of the dataset. And the three rules with most coverage covers 1282 instances which are a 40.11\% of the dataset.
The accuracy yielded in the training dataset was 100\% and the coverage is 100\%.

\lstinputlisting[]{../../models/kr-vs-kp.rules}

\subsection{Car Evaluation Data Set}

The car evaluation dataset yielded 177(only the first 30 rules are presented, all rules can be found on \path{./models/car.rules}) rules in a dataset of 1728 instances. That implies that in average, each rule covers 10 instances which is not a lot. Furthermore, in reality, only the first 11 rules cover 10 or more instances which implies that a lot of rules just cover one instance, which is not ideal as it is overfitting the dataset.
The dataset yielded an accuracy of 100\% and a coverage of 100\% in the training data.
\lstinputlisting[firstline=0, lastline=30]{../../models/car.rules}

\section{Testing Results}

A rule parser and interpreter was defined in \path{./src/rule_interpreter.py} which is able to read a plain text file with the format specified above an create the set of rules and infer in some data the expected labels for each item. the results for the three datasets are presented in \ref{table:results}

\begin{table}[ht]
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|l||c|c|c|c|}
            \hline
            \multirow{2}{5cm}{\textbf{Dataset}} & \multicolumn{2}{c|}{\textbf{Train}} & \multicolumn{2}{c|}{\textbf{Test}}\\
            \cline{2-5}
            & \textbf{accuracy} & \textbf{coverage} & \textbf{accuracy} & \textbf{coverage}\\
            \hline
            \hline
            Chess & 1.0 & 1.0 & 0.9854 & 1.0 \\ \hline
            Car & 1.0 & 1.0 & 0.9 & 1.0 \\ \hline
            Hayes-Roth & 0.8864 & 1.0 & 0.7143 & 1.0 \\ \hline
        \end{tabular}
    }
    \caption{\label{table:results}Results Table}
\end{table}